{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "mount_file_id": "1fJpGqM0xcaCHoVriw1_6nVr5v6O_w580",
   "authorship_tag": "ABX9TyM41HHIscu334R8OBOHOLrB"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Siamese Network \n",
    "https://keras.io/examples/vision/siamese_network/"
   ],
   "metadata": {
    "id": "I_td75BENtIJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1Oz0cYI4MR6d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678101863216,
     "user_tz": -60,
     "elapsed": 10661,
     "user": {
      "displayName": "Bartek",
      "userId": "07317287011393591494"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-01T09:53:17.255131415Z",
     "start_time": "2023-06-01T09:53:16.126986824Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 11:53:16.381669: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-01 11:53:16.903095: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-01 11:53:16.903131: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-01 11:53:16.903135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#Nessesary libs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import resnet\n",
    "from tensorflow.keras.applications import EfficientNetV2B0\n",
    "\n",
    "target_shape = (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cache_dir = Path(Path.home()) / \".keras\"\n",
    "anchor_images_path = cache_dir / \"left\"\n",
    "positive_images_path = cache_dir / \"right\"\n",
    "cache_dir"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEkksrMfv2OC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678101863217,
     "user_tz": -60,
     "elapsed": 12,
     "user": {
      "displayName": "Bartek",
      "userId": "07317287011393591494"
     }
    },
    "outputId": "59284366-c950-45f7-d475-3ef1964df54f",
    "ExecuteTime": {
     "end_time": "2023-06-01T09:53:17.259109189Z",
     "start_time": "2023-06-01T09:53:17.256669023Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "PosixPath('/home/bartlomiej/.keras')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade --no-cache-dir gdown\n",
    "!gdown 1jvkbTr_giSP3Ru8OwGNCg6B4PvVbcO34\n",
    "!gdown 1EzBZUb_mh_Dp_FKD0P4XiYYSd0QBH5zW\n",
    "!unzip -oq left.zip -d $cache_dir\n",
    "!unzip -oq right.zip -d $cache_dir"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vAnbHK-IwIM0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678101876734,
     "user_tz": -60,
     "elapsed": 13525,
     "user": {
      "displayName": "Bartek",
      "userId": "07317287011393591494"
     }
    },
    "outputId": "59a264b9-c571-403c-9340-c10167a00e71",
    "ExecuteTime": {
     "end_time": "2023-06-01T09:53:25.061011638Z",
     "start_time": "2023-06-01T09:53:17.260673248Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/bartlomiej/miniconda3/lib/python3.9/site-packages (4.7.1)\r\n",
      "Requirement already satisfied: tqdm in /home/bartlomiej/miniconda3/lib/python3.9/site-packages (from gdown) (4.64.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /home/bartlomiej/miniconda3/lib/python3.9/site-packages (from gdown) (4.11.1)\r\n",
      "Requirement already satisfied: filelock in /home/bartlomiej/miniconda3/lib/python3.9/site-packages (from gdown) (3.10.0)\r\n",
      "Requirement already satisfied: six in /home/bartlomiej/miniconda3/lib/python3.9/site-packages (from gdown) (1.16.0)\r\n",
      "Requirement already satisfied: requests[socks] in /home/bartlomiej/miniconda3/lib/python3.9/site-packages (from gdown) (2.28.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/bartlomiej/miniconda3/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/bartlomiej/miniconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bartlomiej/miniconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bartlomiej/miniconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bartlomiej/miniconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (2023.5.7)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/bartlomiej/miniconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\r\n",
      "Downloading...\r\n",
      "From (uriginal): https://drive.google.com/uc?id=1jvkbTr_giSP3Ru8OwGNCg6B4PvVbcO34\r\n",
      "From (redirected): https://drive.google.com/uc?id=1jvkbTr_giSP3Ru8OwGNCg6B4PvVbcO34&confirm=t&uuid=ed73b72f-eced-4c83-942d-87925707f9d9\r\n",
      "To: /home/bartlomiej/PycharmProjects/pythonProject/left.zip\r\n",
      "100%|████████████████████████████████████████| 104M/104M [00:01<00:00, 85.8MB/s]\r\n",
      "Downloading...\r\n",
      "From (uriginal): https://drive.google.com/uc?id=1EzBZUb_mh_Dp_FKD0P4XiYYSd0QBH5zW\r\n",
      "From (redirected): https://drive.google.com/uc?id=1EzBZUb_mh_Dp_FKD0P4XiYYSd0QBH5zW&confirm=t&uuid=e0532971-2ebb-4025-b78a-31d4af75870e\r\n",
      "To: /home/bartlomiej/PycharmProjects/pythonProject/right.zip\r\n",
      "100%|████████████████████████████████████████| 104M/104M [00:01<00:00, 90.1MB/s]\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_image(filename):\n",
    "    \"\"\"\n",
    "    Load the specified file as a JPEG image, preprocess it and\n",
    "    resize it to the target shape.\n",
    "    \"\"\"\n",
    "\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, target_shape)\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess_triplets(anchor, positive, negative):\n",
    "    \"\"\"\n",
    "    Given the filenames corresponding to the three images, load and\n",
    "    preprocess them.\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        preprocess_image(anchor),\n",
    "        preprocess_image(positive),\n",
    "        preprocess_image(negative),\n",
    "    )"
   ],
   "metadata": {
    "id": "6sXXQDdJwYZY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678101876735,
     "user_tz": -60,
     "elapsed": 13,
     "user": {
      "displayName": "Bartek",
      "userId": "07317287011393591494"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-01T09:53:25.066343226Z",
     "start_time": "2023-06-01T09:53:25.064625155Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "anchor_images = sorted(\n",
    "    [str(anchor_images_path / f) for f in os.listdir(anchor_images_path)]\n",
    ")\n",
    "\n",
    "positive_images = sorted(\n",
    "    [str(positive_images_path / f) for f in os.listdir(positive_images_path)]\n",
    ")\n",
    "\n",
    "image_count = len(anchor_images)\n",
    "\n",
    "anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
    "positive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)\n",
    "\n",
    "# To generate the list of negative images, let's randomize the list of\n",
    "# available images and concatenate them together.\n",
    "rng = np.random.RandomState(seed=42)\n",
    "rng.shuffle(anchor_images)\n",
    "rng.shuffle(positive_images)\n",
    "\n",
    "negative_images = anchor_images + positive_images\n",
    "np.random.RandomState(seed=32).shuffle(negative_images)\n",
    "\n",
    "negative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)\n",
    "negative_dataset = negative_dataset.shuffle(buffer_size=4096)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
    "dataset = dataset.shuffle(buffer_size=20000)\n",
    "dataset = dataset.map(preprocess_triplets)\n",
    "\n",
    "# Let's now split our dataset in train and validation.\n",
    "train_dataset = dataset.take(round(image_count * 0.8))\n",
    "val_dataset = dataset.skip(round(image_count * 0.8))\n",
    "\n",
    "train_dataset = train_dataset.batch(32, drop_remainder=False)\n",
    "train_dataset = train_dataset.prefetch(8)\n",
    "\n",
    "val_dataset = val_dataset.batch(32, drop_remainder=False)\n",
    "val_dataset = val_dataset.prefetch(8)"
   ],
   "metadata": {
    "id": "u63Wjr-dwbzI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678101877141,
     "user_tz": -60,
     "elapsed": 417,
     "user": {
      "displayName": "Bartek",
      "userId": "07317287011393591494"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-01T09:53:25.570214824Z",
     "start_time": "2023-06-01T09:53:25.070209948Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 11:53:25.177370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:53:25.180475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:53:25.180589: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:53:25.181174: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-01 11:53:25.181747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:53:25.181901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:53:25.182045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:53:25.470145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:53:25.470336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:53:25.470436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:53:25.470519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:07:00.0, compute capability: 6.1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "image_count"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYtLWoJFqS3_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678101920890,
     "user_tz": -60,
     "elapsed": 261,
     "user": {
      "displayName": "Bartek",
      "userId": "07317287011393591494"
     }
    },
    "outputId": "1359314b-7065-4333-d458-059d6c6520a2",
    "ExecuteTime": {
     "end_time": "2023-06-01T09:53:25.573334682Z",
     "start_time": "2023-06-01T09:53:25.570686853Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "6016"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize(anchor, positive, negative):\n",
    "    \"\"\"Visualize a few triplets from the supplied batches.\"\"\"\n",
    "\n",
    "    def show(ax, image):\n",
    "        ax.imshow(image)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "\n",
    "    axs = fig.subplots(3, 3)\n",
    "    for i in range(3):\n",
    "        show(axs[i, 0], anchor[i])\n",
    "        show(axs[i, 1], positive[i])\n",
    "        show(axs[i, 2], negative[i])\n",
    "\n",
    "\n",
    "# visualize(*list(train_dataset.take(1).as_numpy_iterator())[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "NGFoatAZy5_4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1677420955198,
     "user_tz": -60,
     "elapsed": 2288,
     "user": {
      "displayName": "Bartek",
      "userId": "07317287011393591494"
     }
    },
    "outputId": "996f66a0-4949-411f-e3c3-81952569d19c",
    "ExecuteTime": {
     "end_time": "2023-06-01T09:53:25.582984010Z",
     "start_time": "2023-06-01T09:53:25.573636847Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# base_cnn = resnet.ResNet50(\n",
    "#     weights=\"imagenet\", input_shape=target_shape + (3,), include_top=False\n",
    "# )\n",
    "base_cnn = EfficientNetV2B0(\n",
    "    weights=\"imagenet\", input_shape=target_shape + (3,), include_top=False\n",
    ")\n",
    "flatten = layers.Flatten()(base_cnn.output)\n",
    "dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
    "dense1 = layers.BatchNormalization()(dense1)\n",
    "dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n",
    "dense2 = layers.BatchNormalization()(dense2)\n",
    "output = layers.Dense(256)(dense2)\n",
    "\n",
    "embedding = Model(base_cnn.input, output, name=\"Embedding\")\n",
    "\n",
    "trainable = False\n",
    "for layer in base_cnn.layers:\n",
    "    if layer.name == \"conv5_block1_out\":\n",
    "        trainable = True\n",
    "    layer.trainable = trainable"
   ],
   "metadata": {
    "id": "3cc3Q92Oy-5_",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-01T09:53:25.583975228Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tf.keras.losses.cosine_similarity([-1.0,0.0], [1.0,0.0]).numpy()+1)\n",
    "print(tf.reduce_sum(tf.square(np.array([-1.0,0.0]) - np.array([1.0,0.0])), -1).numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import keras.backend\n",
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "\n",
    "        return ap_distance, an_distance\n",
    "\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=target_shape + (3,))\n",
    "positive_input = layers.Input(name=\"positive\", shape=target_shape + (3,))\n",
    "negative_input = layers.Input(name=\"negative\", shape=target_shape + (3,))\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(tf.keras.applications.efficientnet.preprocess_input(anchor_input)),\n",
    "    embedding(tf.keras.applications.efficientnet.preprocess_input(positive_input)),\n",
    "    embedding(tf.keras.applications.efficientnet.preprocess_input(negative_input)),\n",
    ")\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")"
   ],
   "metadata": {
    "id": "xOigHGuGzM1n",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        # loss = ap_distance - an_distance\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ],
   "metadata": {
    "id": "OQKhKPP2zQ79",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer=optimizers.Adam(0.0001))\n",
    "siamese_model.fit(train_dataset, epochs=40, validation_data=val_dataset)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvGKLlL5zlVV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1677429053683,
     "user_tz": -60,
     "elapsed": 8089457,
     "user": {
      "displayName": "Bartek",
      "userId": "07317287011393591494"
     }
    },
    "outputId": "77d1b6d7-f059-40f1-85b9-217c41b69960",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sample = next(iter(train_dataset))\n",
    "# visualize(*sample)\n",
    "\n",
    "anchor, positive, negative = sample\n",
    "anchor_embedding, positive_embedding, negative_embedding = (\n",
    "    embedding(resnet.preprocess_input(anchor)),\n",
    "    embedding(resnet.preprocess_input(positive)),\n",
    "    embedding(resnet.preprocess_input(negative)),\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "3DDipAD_ztKt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1677429076613,
     "user_tz": -60,
     "elapsed": 22933,
     "user": {
      "displayName": "Bartek",
      "userId": "07317287011393591494"
     }
    },
    "outputId": "5a7cb528-af06-42bb-b3d2-550f27c9a5f2",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cosine_similarity = metrics.CosineSimilarity()\n",
    "\n",
    "positive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\n",
    "print(\"Positive similarity:\", positive_similarity.numpy())\n",
    "\n",
    "negative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\n",
    "print(\"Negative similarity\", negative_similarity.numpy())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SleUfatpzvkW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1677429076614,
     "user_tz": -60,
     "elapsed": 10,
     "user": {
      "displayName": "Bartek",
      "userId": "07317287011393591494"
     }
    },
    "outputId": "1d7f4bda-a647-43ad-ab36-01190e7b0996",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#MobileNet EfficientNet"
   ],
   "metadata": {
    "id": "9HZbpToYtwFj",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = next(iter(train_dataset))\n",
    "print(iter(train_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, concatenate\n",
    "\n",
    "# Define the input shape and embedding dimensionality\n",
    "input_shape = (128,)\n",
    "embedding_dim = 64\n",
    "\n",
    "# Define the base network architecture\n",
    "input_layer = Input(shape=input_shape)\n",
    "dense_layer_1 = Dense(32, activation='relu')(input_layer)\n",
    "dense_layer_2 = Dense(embedding_dim, activation='relu')(dense_layer_1)\n",
    "base_network = Model(inputs=input_layer, outputs=dense_layer_2)\n",
    "\n",
    "# Define the siamese network architecture\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "merged_layer = concatenate([processed_a, processed_b])\n",
    "dense_layer_3 = Dense(16, activation='relu')(merged_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(dense_layer_3)\n",
    "siamese_network = Model(inputs=[input_a, input_b], outputs=output_layer)\n",
    "\n",
    "siamese_network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "siamese_network.fit([input_pairs[:, 0], input_pairs[:, 1]], labels, epochs=10, batch_size=16)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
